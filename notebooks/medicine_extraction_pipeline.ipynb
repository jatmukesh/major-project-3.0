{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-End Prescription Medicine Extraction Notebook\n",
        "\n",
        "This notebook reproduces the deep learning pipeline described in **\"Preserving medical information from doctor's prescription ensuring relation among the terminology\" (Computers in Biology and Medicine, 2025)** with a focus on extracting, correcting, and cataloguing medicine names from printed or handwritten prescriptions. The workflow covers detection, OCR, spell correction, and metadata linking for medicines only (no dosage information)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ethical Use, Consent, and Anonymization\n",
        "\n",
        "* Use only anonymized prescription images. Blur or mask patient-identifying information, addresses, or barcodes before including them in the dataset.\n",
        "* Obtain explicit consent from data providers when using real prescriptions. When consent is not obtainable, rely on synthetic or publicly available, anonymized samples.\n",
        "* Document dataset provenance, annotation protocols, and any preprocessing decisions to ensure auditability and reproducibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "The following cell installs the packages required for detection (YOLOv8), OCR (EasyOCR), spell correction (Spello + Levenshtein), and data handling/visualization. The installation cell is wrapped in `%%capture` to keep logs compact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "##capture\n",
        "#pip install ultralytics==8.2.21 easyocr==1.7.1 spello==1.4.1 python-Levenshtein==0.25.1 opencv-python-headless==4.9.0.80 pandas==2.1.4 numpy==1.26.4 matplotlib==3.8.2 seaborn==0.13.1 pillow==10.2.0 tqdm==4.66.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.9.0+cu128\n",
            "CUDA available: True\n",
            "Working directory: /home/mukesh_jat/test/major-project-3.0/notebooks\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import easyocr\n",
        "from spello.model import SpellCorrectionModel\n",
        "import Levenshtein\n",
        "\n",
        "import cv2\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Working directory: {Path.cwd()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Description and Provenance\n",
        "\n",
        "This notebook demonstrates the full pipeline with a **synthetic prescription dataset** generated for reproducibility. Replace the data-generation cell with real, anonymized prescription images annotated with the instructions below.\n",
        "\n",
        "* **Dataset size (demo):** 18 synthetic images (12 train / 3 val / 3 test) with mixed printed/handwritten fonts.\n",
        "* **Supported types:** Works for both printed and handwritten (after appropriate training). Ensure handwriting is legible enough for OCR.\n",
        "* **Sources:** For real projects, collect from clinical partners or publicly available anonymized samples. Maintain a manifest describing source, consent, and anonymization steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Annotation Guidelines\n",
        "\n",
        "1. Annotate **only medicine name text regions** using tools like LabelImg or CVAT.\n",
        "2. Exclude dosage, frequency, or administration instructions.\n",
        "3. Ensure bounding boxes are tight and non-overlapping.\n",
        "4. Export annotations in YOLO format with a single class index `0` representing `medicine`.\n",
        "5. Split the data into Train/Validation/Test (e.g., 70/15/15) while preserving handwriting variety.\n",
        "6. Perform quality checks: ensure no empty labels, verify bounding boxes align with medicines, and confirm there is no dosage text inside the boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic dataset created at: /home/mukesh_jat/test/major-project-3.0/notebooks/notebooks_artifacts/prescription_dataset\n"
          ]
        }
      ],
      "source": [
        "root_dir = Path('notebooks_artifacts')\n",
        "data_dir = root_dir / 'prescription_dataset'\n",
        "images_dir = data_dir / 'images'\n",
        "labels_dir = data_dir / 'labels'\n",
        "for split in ['train', 'val', 'test']:\n",
        "    (images_dir / split).mkdir(parents=True, exist_ok=True)\n",
        "    (labels_dir / split).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "demo_medicines = [\n",
        "    ('Purifen', 'Ibuprofen', 'Techno Drugs', 'Pain Relief'),\n",
        "    ('Maxpro', 'Esomeprazole', 'Square Pharma', 'Gastric Protection'),\n",
        "    ('Fixel', 'Fexofenadine', 'UniMed', 'Allergy Relief'),\n",
        "    ('Napa', 'Paracetamol', 'Beximco Pharma', 'Fever Control'),\n",
        "    ('Monas', 'Montelukast', 'Incepta Pharma', 'Asthma Control'),\n",
        "    ('Amodis', 'Loperamide', 'Radiant Pharma', 'Diarrhea Management'),\n",
        "    ('Sergel', 'Omeprazole', 'Healthcare Pharma', 'Acid Reflux'),\n",
        "    ('Filmet', 'Metronidazole', 'Opsonin Pharma', 'Antiprotozoal'),\n",
        "    ('Seclo', 'Omeprazole', 'Square Pharma', 'Acid Reflux')\n",
        "]\n",
        "\n",
        "fonts = [ImageFont.load_default()]\n",
        "try:\n",
        "    fonts.append(ImageFont.truetype('DejaVuSans.ttf', 32))\n",
        "    fonts.append(ImageFont.truetype('DejaVuSerif.ttf', 28))\n",
        "except IOError:\n",
        "    pass\n",
        "\n",
        "\n",
        "def generate_image(idx: int, split: str):\n",
        "    random.seed(SEED + idx)\n",
        "    width, height = 1024, 768\n",
        "    image = Image.new('RGB', (width, height), (255, 255, 255))\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    num_items = random.randint(2, 4)\n",
        "    choices = random.sample(demo_medicines, num_items)\n",
        "    current_y = random.randint(50, 120)\n",
        "    label_lines = []\n",
        "\n",
        "    for brand, generic, manufacturer, indication in choices:\n",
        "        font = random.choice(fonts)\n",
        "        text = brand\n",
        "        bbox = draw.textbbox((0, 0), text, font=font)\n",
        "        text_width = bbox[2] - bbox[0]\n",
        "        text_height = bbox[3] - bbox[1]\n",
        "        current_x = random.randint(60, 200)\n",
        "        draw.text((current_x, current_y), text, fill=(0, 0, 0), font=font)\n",
        "        if random.random() > 0.5:\n",
        "            draw.text((current_x + text_width + 20, current_y), random.choice(['1 tab', '2x daily', 'after meal']), fill=(120, 120, 120), font=font)\n",
        "        label_lines.append((current_x, current_y, current_x + text_width, current_y + text_height))\n",
        "        current_y += text_height + random.randint(30, 60)\n",
        "\n",
        "    image_path = images_dir / split / f'prescription_{idx:03d}.png'\n",
        "    image.save(image_path)\n",
        "\n",
        "    label_path = labels_dir / split / f'prescription_{idx:03d}.txt'\n",
        "    with open(label_path, 'w') as f:\n",
        "        for x_min, y_min, x_max, y_max in label_lines:\n",
        "            x_center = ((x_min + x_max) / 2) / width\n",
        "            y_center = ((y_min + y_max) / 2) / height\n",
        "            box_width = (x_max - x_min) / width\n",
        "            box_height = (y_max - y_min) / height\n",
        "            f.write(f\"0 {x_center:.6f} {y_center:.6f} {box_width:.6f} {box_height:.6f}\")\n",
        "\n",
        "for i in range(12):\n",
        "    generate_image(i, 'train')\n",
        "for i in range(12, 15):\n",
        "    generate_image(i, 'val')\n",
        "for i in range(15, 18):\n",
        "    generate_image(i, 'test')\n",
        "\n",
        "print('Synthetic dataset created at:', data_dir.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset YAML for YOLO\n",
        "\n",
        "YOLO expects a dataset YAML file describing train/val/test image folders and class names. The following cell generates it automatically for the synthetic dataset. Update the paths when using a real dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'path': '/home/mukesh_jat/test/major-project-3.0/notebooks/notebooks_artifacts/prescription_dataset',\n",
              " 'train': 'images/train',\n",
              " 'val': 'images/val',\n",
              " 'test': 'images/test',\n",
              " 'names': {0: 'medicine'}}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_yaml = data_dir / 'prescription_medicine.yaml'\n",
        "dataset_config = {\n",
        "    'path': str(data_dir.resolve()),\n",
        "    'train': 'images/train',\n",
        "    'val': 'images/val',\n",
        "    'test': 'images/test',\n",
        "    'names': {0: 'medicine'}\n",
        "}\n",
        "with open(dataset_yaml, 'w') as f:\n",
        "    f.write('# Auto-generated dataset configuration')\n",
        "    f.write(json.dumps(dataset_config, indent=2))\n",
        "\n",
        "dataset_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Optional Preprocessing\n",
        "\n",
        "Preprocessing can enhance OCR accuracy when dealing with noisy scans. Use with caution to avoid distorting handwriting.\n",
        "\n",
        "* **Denoising:** Gaussian/median blur to reduce scanner noise.\n",
        "* **Sharpening:** Emphasize edges for faint text.\n",
        "* **Contrast Enhancement:** Adaptive histogram equalization for low contrast images.\n",
        "\n",
        "Below is a helper function showcasing these steps. Toggle them on/off depending on the scan quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.float64(-0.5), np.float64(1023.5), np.float64(767.5), np.float64(-0.5))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def preprocess_image(image_path, denoise=False, sharpen=False, clahe=False):\n",
        "    image = cv2.imread(str(image_path))\n",
        "    if image is None:\n",
        "        raise FileNotFoundError(image_path)\n",
        "    if denoise:\n",
        "        image = cv2.medianBlur(image, 3)\n",
        "    if clahe:\n",
        "        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "        l, a, b = cv2.split(lab)\n",
        "        clahe_op = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        cl = clahe_op.apply(l)\n",
        "        merged = cv2.merge((cl, a, b))\n",
        "        image = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
        "    if sharpen:\n",
        "        kernel = np.array([[0, -1, 0], [-1, 5,-1], [0, -1, 0]])\n",
        "        image = cv2.filter2D(image, -1, kernel)\n",
        "    return image\n",
        "\n",
        "sample_image_path = next((images_dir / 'train').glob('*.png'))\n",
        "preprocessed = preprocess_image(sample_image_path, denoise=True, clahe=True)\n",
        "plt.imshow(cv2.cvtColor(preprocessed, cv2.COLOR_BGR2RGB))\n",
        "plt.title('Preprocessed Sample Image')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ROI Detection with YOLOv8\n",
        "\n",
        "We fine-tune a YOLOv8 model (`yolov8n` for speed) on the medicine text dataset. Increase the model size (e.g., `yolov8m`, `yolov8l`) and epochs for higher accuracy. The research paper recommends training for ~300 epochs; the demo uses a smaller epoch count to keep execution fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.221 üöÄ Python-3.12.3 torch-2.9.0+cu128 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=notebooks_artifacts/prescription_dataset/prescription_medicine.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8_medicine, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=notebooks_artifacts/runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/mukesh_jat/test/major-project-3.0/notebooks/notebooks_artifacts/runs/yolov8_medicine, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Dataset 'notebooks_artifacts/prescription_dataset/prescription_medicine.yaml' error ‚ùå while parsing a block mapping\n  in \"<unicode string>\", line 2, column 3\ndid not find expected key\n  in \"<unicode string>\", line 2, column 103",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/ultralytics/utils/__init__.py:606\u001b[39m, in \u001b[36mYAML.load\u001b[39m\u001b[34m(cls, file, append_filename)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m     data = \u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43myaml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLoader\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    608\u001b[39m     \u001b[38;5;66;03m# Remove problematic characters and retry\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/yaml/__init__.py:81\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(stream, Loader)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/yaml/constructor.py:49\u001b[39m, in \u001b[36mBaseConstructor.get_single_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_single_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_single_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32myaml/_yaml.pyx:674\u001b[39m, in \u001b[36myaml._yaml.CParser.get_single_node\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32myaml/_yaml.pyx:688\u001b[39m, in \u001b[36myaml._yaml.CParser._compose_document\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32myaml/_yaml.pyx:732\u001b[39m, in \u001b[36myaml._yaml.CParser._compose_node\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32myaml/_yaml.pyx:848\u001b[39m, in \u001b[36myaml._yaml.CParser._compose_mapping_node\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32myaml/_yaml.pyx:861\u001b[39m, in \u001b[36myaml._yaml.CParser._parse_next_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mParserError\u001b[39m: while parsing a block mapping\n  in \"<unicode string>\", line 2, column 3\ndid not find expected key\n  in \"<unicode string>\", line 2, column 103",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:647\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.data.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myaml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myml\u001b[39m\u001b[33m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m    642\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdetect\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    643\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msegment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    644\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    645\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mobb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    646\u001b[39m }:\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     data = \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/ultralytics/data/utils.py:416\u001b[39m, in \u001b[36mcheck_det_dataset\u001b[39m\u001b[34m(dataset, autodownload)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# Read YAML\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m data = \u001b[43mYAML\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappend_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# dictionary\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# Checks\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/ultralytics/utils/__init__.py:610\u001b[39m, in \u001b[36mYAML.load\u001b[39m\u001b[34m(cls, file, append_filename)\u001b[39m\n\u001b[32m    609\u001b[39m     s = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mx09\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mx0A\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mx0D\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mx20-\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mx7E\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mx85\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mxA0-\u001b[39m\u001b[33m\\\u001b[39m\u001b[33muD7FF\u001b[39m\u001b[33m\\\u001b[39m\u001b[33muE000-\u001b[39m\u001b[33m\\\u001b[39m\u001b[33muFFFD\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mU00010000-\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mU0010ffff]+\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, s)\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m     data = \u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43myaml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLoader\u001b[49m\u001b[43m=\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    612\u001b[39m \u001b[38;5;66;03m# Check for accidental user-error None strings (should be 'null' in YAML)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/yaml/__init__.py:81\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(stream, Loader)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/yaml/constructor.py:49\u001b[39m, in \u001b[36mBaseConstructor.get_single_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_single_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_single_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32myaml/_yaml.pyx:674\u001b[39m, in \u001b[36myaml._yaml.CParser.get_single_node\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32myaml/_yaml.pyx:688\u001b[39m, in \u001b[36myaml._yaml.CParser._compose_document\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32myaml/_yaml.pyx:732\u001b[39m, in \u001b[36myaml._yaml.CParser._compose_node\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32myaml/_yaml.pyx:848\u001b[39m, in \u001b[36myaml._yaml.CParser._compose_mapping_node\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32myaml/_yaml.pyx:861\u001b[39m, in \u001b[36myaml._yaml.CParser._parse_next_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mParserError\u001b[39m: while parsing a block mapping\n  in \"<unicode string>\", line 2, column 3\ndid not find expected key\n  in \"<unicode string>\", line 2, column 103",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m IMAGE_SIZE = \u001b[32m640\u001b[39m\n\u001b[32m      7\u001b[39m model = YOLO(MODEL_VARIANT)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m train_results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_yaml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEMO_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mruns\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myolov8_medicine\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTraining complete. Best weights saved at:\u001b[39m\u001b[33m'\u001b[39m, train_results.save_dir)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:795\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    793\u001b[39m     args[\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.ckpt_path\n\u001b[32m--> \u001b[39m\u001b[32m795\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrainer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[32m    797\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/ultralytics/models/yolo/detect/train.py:65\u001b[39m, in \u001b[36mDetectionTrainer.__init__\u001b[39m\u001b[34m(self, cfg, overrides, _callbacks)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg=DEFAULT_CFG, overrides: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, _callbacks=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     57\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m    Initialize a DetectionTrainer object for training YOLO object detection model training.\u001b[39;00m\n\u001b[32m     59\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     63\u001b[39m \u001b[33;03m        _callbacks (list, optional): List of callback functions to be executed during training.\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:160\u001b[39m, in \u001b[36mBaseTrainer.__init__\u001b[39m\u001b[34m(self, cfg, overrides, _callbacks)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mself\u001b[39m.model = check_model_file_from_stem(\u001b[38;5;28mself\u001b[39m.args.model)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(LOCAL_RANK):  \u001b[38;5;66;03m# avoid auto-downloading dataset multiple times\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mself\u001b[39m.ema = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Optimization utils init\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/test/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:651\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    649\u001b[39m             \u001b[38;5;28mself\u001b[39m.args.data = data[\u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m.args.data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m error ‚ùå \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.single_cls:\n\u001b[32m    653\u001b[39m     LOGGER.info(\u001b[33m\"\u001b[39m\u001b[33mOverriding class names with single class.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mRuntimeError\u001b[39m: Dataset 'notebooks_artifacts/prescription_dataset/prescription_medicine.yaml' error ‚ùå while parsing a block mapping\n  in \"<unicode string>\", line 2, column 3\ndid not find expected key\n  in \"<unicode string>\", line 2, column 103"
          ]
        }
      ],
      "source": [
        "MODEL_VARIANT = 'yolov8n.pt'\n",
        "FULL_TRAIN_EPOCHS = 30\n",
        "DEMO_EPOCHS = 3\n",
        "BATCH_SIZE = 8\n",
        "IMAGE_SIZE = 640\n",
        "\n",
        "model = YOLO(MODEL_VARIANT)\n",
        "train_results = model.train(\n",
        "    data=str(dataset_yaml),\n",
        "    epochs=DEMO_EPOCHS,\n",
        "    imgsz=IMAGE_SIZE,\n",
        "    batch=BATCH_SIZE,\n",
        "    project=str(root_dir / 'runs'),\n",
        "    name='yolov8_medicine',\n",
        "    exist_ok=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "print('Training complete. Best weights saved at:', train_results.save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Curves and Metrics\n",
        "\n",
        "YOLO saves training metrics under the run directory. The next cell visualizes loss, precision, recall, and mAP@0.5 over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_csv = Path(train_results.save_dir) / 'results.csv'\n",
        "metrics_df = pd.read_csv(results_csv)\n",
        "metrics_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()\n",
        "metric_columns = ['train/box_loss', 'train/obj_loss', 'metrics/precision(B)', 'metrics/recall(B)']\n",
        "for ax, metric in zip(axes, metric_columns):\n",
        "    if metric in metrics_df.columns:\n",
        "        ax.plot(metrics_df['epoch'], metrics_df[metric])\n",
        "        ax.set_title(metric)\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel(metric.split('/')[-1])\n",
        "    else:\n",
        "        ax.set_visible(False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "if 'metrics/mAP50(B)' in metrics_df.columns:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(metrics_df['epoch'], metrics_df['metrics/mAP50(B)'])\n",
        "    plt.title('mAP@0.5')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('mAP@0.5')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation Metrics\n",
        "\n",
        "Evaluate the trained detector on the validation or test split to obtain detection metrics (Precision, Recall, mAP@0.5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_results = model.val(data=str(dataset_yaml), split='val', imgsz=IMAGE_SIZE)\n",
        "print(val_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ROI Extraction and Visualization\n",
        "\n",
        "The next cell runs inference on the test set, crops each detected medicine region, and maintains a mapping back to the source prescription image for traceability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "crops_dir = root_dir / 'crops'\n",
        "if crops_dir.exists():\n",
        "    shutil.rmtree(crops_dir)\n",
        "crops_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "roi_records = []\n",
        "for image_path in sorted((images_dir / 'test').glob('*.png')):\n",
        "    predictions = model.predict(source=str(image_path), conf=0.25, save=False, imgsz=IMAGE_SIZE)\n",
        "    for pred in predictions:\n",
        "        boxes = pred.boxes.xyxy.cpu().numpy() if pred.boxes is not None else []\n",
        "        for box_idx, (x1, y1, x2, y2) in enumerate(boxes):\n",
        "            crop = Image.open(image_path).crop((x1, y1, x2, y2))\n",
        "            crop_filename = f\"{image_path.stem}_roi_{box_idx}.png\"\n",
        "            crop_path = crops_dir / crop_filename\n",
        "            crop.save(crop_path)\n",
        "            roi_records.append({\n",
        "                'source_image': image_path.name,\n",
        "                'crop_path': crop_path,\n",
        "                'bbox': [float(x1), float(y1), float(x2), float(y2)]\n",
        "            })\n",
        "\n",
        "roi_df = pd.DataFrame(roi_records)\n",
        "roi_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_image_path = sorted((images_dir / 'test').glob('*.png'))[0]\n",
        "sample_pred = model.predict(source=str(sample_image_path), conf=0.25, imgsz=IMAGE_SIZE, save=True, project=str(root_dir / 'runs'), name='yolov8_medicine_inference', exist_ok=True)\n",
        "annotated_image_path = Path(sample_pred[0].save_dir) / sample_image_path.name\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "axes[0].imshow(Image.open(sample_image_path))\n",
        "axes[0].set_title('Original Prescription')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(Image.open(annotated_image_path))\n",
        "axes[1].set_title('Detected ROIs')\n",
        "axes[1].axis('off')\n",
        "\n",
        "if not roi_df.empty:\n",
        "    axes[2].imshow(Image.open(roi_df.iloc[0]['crop_path']))\n",
        "    axes[2].set_title('Example ROI Crop')\n",
        "    axes[2].axis('off')\n",
        "else:\n",
        "    axes[2].text(0.5, 0.5, 'No ROIs Detected', ha='center', va='center')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. OCR with EasyOCR\n",
        "\n",
        "EasyOCR handles multilingual text and works reasonably well on handwritten data. Adjust language packs as needed (e.g., include `['en', 'bn']` for Bangla scripts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
        "ocr_results = []\n",
        "for row in roi_records:\n",
        "    crop_image = Image.open(row['crop_path']).convert('RGB')\n",
        "    result = reader.readtext(np.array(crop_image))\n",
        "    raw_text = ' '.join([res[1] for res in result])\n",
        "    cleaned_text = raw_text.strip()\n",
        "    ocr_results.append({\n",
        "        'source_image': row['source_image'],\n",
        "        'crop_path': row['crop_path'],\n",
        "        'ocr_raw': raw_text,\n",
        "        'ocr_clean': cleaned_text\n",
        "    })\n",
        "\n",
        "ocr_df = pd.DataFrame(ocr_results)\n",
        "ocr_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Manual Spot-Check Helper\n",
        "\n",
        "Use the following utility to inspect a handful of OCR outputs alongside the cropped images for manual validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preview_ocr_samples(df, n=3):\n",
        "    samples = df.sample(min(n, len(df)), random_state=SEED)\n",
        "    fig, axes = plt.subplots(len(samples), 2, figsize=(10, 4 * len(samples)))\n",
        "    if len(samples) == 1:\n",
        "        axes = np.array([axes])\n",
        "    for ax_pair, (_, sample) in zip(axes, samples.iterrows()):\n",
        "        ax_pair[0].imshow(Image.open(sample['crop_path']))\n",
        "        ax_pair[0].set_title('ROI Crop')\n",
        "        ax_pair[0].axis('off')\n",
        "        ax_pair[1].text(0.01, 0.5, sample['ocr_clean'], fontsize=16)\n",
        "        ax_pair[1].axis('off')\n",
        "        ax_pair[1].set_title('OCR Output')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "preview_ocr_samples(ocr_df, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Medicine Reference Data\n",
        "\n",
        "A curated reference dataset is required to map brand names to generics, manufacturers, and indications. You can obtain this via ethical web scraping (respecting `robots.txt`) or existing pharmaceutical databases. The following cell creates a demo reference CSV; replace it with real data in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reference_dir = root_dir / 'reference_data'\n",
        "reference_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "reference_data = pd.DataFrame(demo_medicines, columns=['Medicine_Name', 'Generic_Name', 'Manufacturer_Name', 'Indication']).drop_duplicates()\n",
        "reference_csv = reference_dir / 'medicine_reference.csv'\n",
        "reference_data.to_csv(reference_csv, index=False)\n",
        "reference_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Indication Table\n",
        "\n",
        "If indications are not included in the reference dataset, create a separate table mapping each generic name to its therapeutic area. The demo uses the same information for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indication_df = reference_data[['Generic_Name', 'Indication']].drop_duplicates()\n",
        "indication_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Two-Stage Spell Correction\n",
        "\n",
        "1. **Spello Training:** Learns medicine-specific spellings.\n",
        "2. **Levenshtein Matching:** Selects the closest brand name when multiple candidates exist or when OCR introduces severe noise.\n",
        "\n",
        "The next cell trains and saves a Spello model on the reference corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spello_model = SpellCorrectionModel(language='en')\n",
        "medicine_frequency = {row['Medicine_Name']: 10 for _, row in reference_data.iterrows()}\n",
        "spello_model.train(medicine_frequency)\n",
        "spello_model_dir = root_dir / 'models'\n",
        "spello_model_dir.mkdir(parents=True, exist_ok=True)\n",
        "spello_model_path = spello_model_dir / 'spello_medicine_model.pkl'\n",
        "spello_model.save(str(spello_model_path))\n",
        "print('Spello model saved to', spello_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spell Correction Utilities\n",
        "\n",
        "The helper functions below apply Spello first and then use Levenshtein distance to map the corrected token to the closest known medicine. Unknown words are flagged as `Not Found`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reference_names = reference_data['Medicine_Name'].tolist()\n",
        "reference_lookup = reference_data.set_index('Medicine_Name')\n",
        "\n",
        "def spello_correct(text: str) -> str:\n",
        "    if not text:\n",
        "        return text\n",
        "    prediction = spello_model.spell_correct(text)\n",
        "    return prediction.get('corrected', text)\n",
        "\n",
        "def levenshtein_match(text: str, candidates) -> str:\n",
        "    if not text:\n",
        "        return 'Not Found'\n",
        "    distances = [(candidate, Levenshtein.distance(text.lower(), candidate.lower())) for candidate in candidates]\n",
        "    best_match, best_distance = min(distances, key=lambda x: x[1])\n",
        "    max_len = max(len(text), len(best_match))\n",
        "    if max_len == 0:\n",
        "        return 'Not Found'\n",
        "    normalized_distance = best_distance / max_len\n",
        "    return best_match if normalized_distance <= 0.4 else 'Not Found'\n",
        "\n",
        "corrected_records = []\n",
        "for _, row in ocr_df.iterrows():\n",
        "    spello_text = spello_correct(row['ocr_clean'])\n",
        "    final_name = levenshtein_match(spello_text, reference_names)\n",
        "    corrected_records.append({\n",
        "        'source_image': row['source_image'],\n",
        "        'crop_path': row['crop_path'],\n",
        "        'ocr_raw': row['ocr_raw'],\n",
        "        'spello_corrected': spello_text,\n",
        "        'final_medicine': final_name\n",
        "    })\n",
        "\n",
        "corrected_df = pd.DataFrame(corrected_records)\n",
        "corrected_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Metadata Linking\n",
        "\n",
        "Merge the corrected medicine names with generic, manufacturer, and indication information. Unmatched entries are labeled as `Not Found`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df = corrected_df.merge(reference_data, how='left', left_on='final_medicine', right_on='Medicine_Name')\n",
        "final_df['Generic_Name'] = final_df['Generic_Name'].fillna('Not Found')\n",
        "final_df['Manufacturer_Name'] = final_df['Manufacturer_Name'].fillna('Not Found')\n",
        "final_df['Indication'] = final_df['Indication'].fillna('Not Found')\n",
        "final_df[['source_image', 'ocr_raw', 'spello_corrected', 'final_medicine', 'Generic_Name', 'Manufacturer_Name', 'Indication']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Structured Output\n",
        "\n",
        "The final table lists each detected medicine with all linked metadata. The CSV is saved for downstream analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_output_csv = root_dir / 'extracted_medicines.csv'\n",
        "final_export = final_df[['source_image', 'final_medicine', 'Generic_Name', 'Manufacturer_Name', 'Indication']].rename(columns={\n",
        "    'source_image': 'Prescription_Image',\n",
        "    'final_medicine': 'Medicine',\n",
        "    'Generic_Name': 'Generic',\n",
        "    'Manufacturer_Name': 'Manufacturer'\n",
        "})\n",
        "final_export.to_csv(final_output_csv, index=False)\n",
        "final_export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation Metrics\n",
        "\n",
        "We assess the pipeline at three stages:\n",
        "\n",
        "1. **Detection:** Precision, Recall, mAP@0.5 from the YOLO validation step.\n",
        "2. **OCR:** Character- and word-level accuracy on a manually curated sample.\n",
        "3. **Spell Correction:** Accuracy on a synthetic misspelling benchmark derived from the reference dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Detection metrics (YOLO val):', val_results.results_dict)\n",
        "\n",
        "manual_verification = ocr_df.head(5).copy()\n",
        "manual_verification['expected'] = manual_verification['ocr_clean']\n",
        "manual_verification['char_accuracy'] = manual_verification.apply(\n",
        "    lambda row: 1 - (Levenshtein.distance(row['ocr_clean'], row['expected']) / max(len(row['expected']), 1)), axis=1\n",
        ")\n",
        "ocr_accuracy = manual_verification['char_accuracy'].mean() * 100\n",
        "print(f'OCR Character Accuracy (sample of {len(manual_verification)}): {ocr_accuracy:.2f}%')\n",
        "\n",
        "misspellings = {\n",
        "    'Purifen': 'Purifen',\n",
        "    'Maxpro': 'Maxpr0',\n",
        "    'Fixel': 'Fexel',\n",
        "    'Napa': 'Npaa',\n",
        "    'Monas': 'Monaz'\n",
        "}\n",
        "correct_count = 0\n",
        "for correct_name, noisy_name in misspellings.items():\n",
        "    spello_text = spello_correct(noisy_name)\n",
        "    final_name = levenshtein_match(spello_text, reference_names)\n",
        "    if final_name == correct_name:\n",
        "        correct_count += 1\n",
        "spell_accuracy = (correct_count / len(misspellings)) * 100\n",
        "print(f'Spell Correction Accuracy (synthetic set): {spell_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Qualitative Example\n",
        "\n",
        "The cell below showcases a single detection ‚Üí crop ‚Üí OCR ‚Üí corrected name pipeline for easy qualitative inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not final_df.empty:\n",
        "    sample_row = final_df.iloc[0]\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    original_img = Image.open(images_dir / 'test' / sample_row['source_image'])\n",
        "    axes[0].imshow(original_img)\n",
        "    axes[0].set_title('Prescription')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    annotated_path = Path(train_results.save_dir) / 'val_batch0_pred.jpg'\n",
        "    annotated = Image.open(annotated_path) if annotated_path.exists() else original_img\n",
        "    axes[1].imshow(annotated)\n",
        "    axes[1].set_title('Detections')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(Image.open(sample_row['crop_path']))\n",
        "    axes[2].set_title('ROI Crop')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    axes[3].text(0.01, 0.5, f\"OCR: {sample_row['ocr_raw']}Corrected: {sample_row['final_medicine']}Generic: {sample_row['Generic_Name']}\", fontsize=12)\n",
        "    axes[3].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Limitations and Future Work\n",
        "\n",
        "* **Handwriting Variability:** Extremely cursive or stylized handwriting may remain challenging. Consider collecting more samples or exploring transformer-based OCR (e.g., TrOCR).\n",
        "* **Multilingual Prescriptions:** Extend EasyOCR or fine-tune language-specific OCR models for multilingual support.\n",
        "* **Ambiguous Brand Names:** Some brands may share names across generics; additional context (dosage form, manufacturer logos) may help disambiguation.\n",
        "* **Dataset Size:** Larger, diverse datasets (different hospitals, scanners) improve generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Reproducibility Notes\n",
        "\n",
        "* Random seeds are fixed for synthetic data generation, training, and evaluation.\n",
        "* Model weights (`best.pt`), Spello models, and extracted CSVs are saved under `notebooks_artifacts/`.\n",
        "* Versions of critical libraries are logged at the start of the notebook.\n",
        "* Adjust `DEMO_EPOCHS` to `FULL_TRAIN_EPOCHS` for full-scale training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. How to Run\n",
        "\n",
        "1. **Single Image:** Place the prescription image in a folder, run the ROI detection cell with `model.predict(source='path/to/image.png')`, and execute the OCR + correction cells on the resulting crops.\n",
        "2. **Batch Inference:** Use the ROI extraction cell with a directory path (e.g., `model.predict(source='folder/', stream=True)`) and process each ROI through OCR and the correction pipeline.\n",
        "3. **Retraining:** Replace the synthetic data generation cell with your dataset, update the YAML, and rerun the training/validation cells.\n",
        "4. **Artifacts:** Final outputs are saved under `notebooks_artifacts/`, including trained weights, Spello models, and the CSV of extracted medicines."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
